<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Behavioral Cloning</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Andre Simpelo's Project Portfolio">
    <meta name="keywords" content="deep learning,machine learning,ai,artificial intelligence,self driving cars,robotics,guitar,jazz" />
    <meta name="author" content="Simpelo Andre">
    <link rel="canonical" href="http://localhost:4000/behavioral_cloning/">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="/style.css">

    <!-- Google verification -->
    

    <!-- Bing Verification -->
    

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet"> 
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond" rel="stylesheet"> 

</head>

  <!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Home</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li class="hidden">
          <a href="#page-top"></a>
        </li>
        <li class="page-scroll">
          <a href="#portfolio">Projects</a>
        </li>
        <li class="page-scroll">
          <a href="mailto:dresimpelo@gmail.com" class="portfolio-link" data-toggle="modal">
            Contact
          </a>
        </li>
        <li class="page-scroll">
          <a href="https://drive.google.com/open?id=0B-ydlfoJ78M0MkcxX2d6cDVUeGc">Resume</a>
        </li>
      </ul>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container-fluid -->
</nav>

    <body>
    

    <!-- Content -->
    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Behavioral Cloning</h1>
  </header>

  <article class="post-content">
  <div class="youtube">
<iframe width="560" height="315" src="https://www.youtube.com/embed/kW9rJdLdAgI?rel=0" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>For this project, we had to predict steering angles given (virtual) dashboard camera images. One way to approach this, would be to get a computer to mimic a professional human driver (me!). This is done by logging corresponding steering angles with their temporally paired camera image. A convolutional neural network is a perfect candidate for dealing with pixel data, so lets begin!</p>

<h2 id="1-model-architecture">1. Model Architecture</h2>

<p>I decided to base my architecture off of the Nvidia End to End Learning for Self-Driving Cars <a href="https://arxiv.org/pdf/1604.07316v1.pdf">paper</a>. My architecture also has some design inspiration from <a href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a> as well as utilization of <a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf">Batch Norm</a> for ease of training and simplicity (Details below).</p>

<p>I’m a fan of the elegant/clean nature of the VGG architecture. I understand it is no longer state of the art, but I still like their philosophy of simplicity.</p>

<p>I’m also a huge fan of Batch Normalization. It makes weight initialization less important, and also improves the quality of the gradient flow during backprop (due to forcing our activations to have a normal distribution) (<a href="http://cs231n.stanford.edu/slides/winter1516_lecture5.pdf">Slide 69 cs231n Lecture 5</a>). I added this in hopes of faster training, and a cleaner gradient flow.</p>

<p><strong>Overview of Architecture:</strong></p>

<ul>
  <li>
    <p>I use rectified linear units as my default choice of activation function throughout my network. It is a battle-proven, easily interpretable and a go to choice for many state of the art networks.</p>
  </li>
  <li>
    <p>I follow each convolutional and fully connected layer with a Batch Norm layer.</p>
  </li>
</ul>

<p>Layer 1: Normalization.</p>
<ul>
  <li>Normalize the pixel data to have a range between 0 and 1.</li>
</ul>

<p>Layer 2-5: Convolutional Layers</p>

<ul>
  <li>
    <p>I decided to follow the Nvidia paper with filter sizes, strides, and number of layers.</p>
  </li>
  <li><strong>Nvidia starts with 3 5x5 filters, with stride 2x2.</strong></li>
  <li>Conv1:
    <ul>
      <li>I start off with 64 5x5 filters, with stride 2x2. I decided to start with 64 filters since it is a clean power of 2. I probably could have gotten away with using 32 filters (as Nvidia uses only 24 filters in their real-life model).</li>
    </ul>
  </li>
  <li>Conv2:
    <ul>
      <li>Similar to Conv1, I use 64 5x5 filters.</li>
    </ul>
  </li>
  <li>Conv3:
    <ul>
      <li>I double the amount of filters to 128 at this layer (<em>inspired by VGG</em>), still using 5x5 filters and 2x2 stride (Nvidia)</li>
    </ul>
  </li>
  <li><strong>Nvidia follows by switching to 3x3 filters, with normal strided convolutions for the last 2 convolutional layers</strong></li>
  <li>Conv4:
    <ul>
      <li>128 3x3 Filters. (Simply keeping the filter count the same as the previous layer)</li>
    </ul>
  </li>
  <li>Conv5:
    <ul>
      <li>256 3x3 Filters. (Doubling the amount of filters of the previous layer)</li>
    </ul>
  </li>
  <li><strong>Nvidia follows the convolutional layers with 2 fully connected layers</strong></li>
  <li>FC 1:
    <ul>
      <li>The previous layer is flattened into a long vector, and used as input to the fully connected layer.</li>
      <li>We have 1024 units in this 1st FC. 1024 is a random number chosen (Ball-park guess of what is enough) to ensure we have enough representaional power for this regression model.</li>
      <li>We use 0.8 Dropout here to prevent overfitting.</li>
    </ul>
  </li>
  <li>FC 2:
    <ul>
      <li>We finish with 256 units, before predicting our steering angle.</li>
    </ul>
  </li>
  <li><strong>Output Predicted Steering Angle</strong></li>
</ul>

<h2 id="2-creation-of-the-training-dataset-and-the-training-process">2. Creation of the training dataset and the training process</h2>

<p>During the process of data acquisition, I was unsure of what kind of data would be needed to successfully drive both tracks. So I recorded a single dataset for each of the tracks, trained it through a few models [all models are based off of the ‘base’ model listed above], and evaluated them through a critical review of its performance through the simulator.</p>

<p>I found that it is very easy to train a model to memorize a single track (either 1 or 2), but it is a bit more challenging to make a model to do well on both tracks. I also did not just want to memorize both tracks, I wanted the car to be as truly autonomous as possible.</p>

<p>I found that simply driving through both tracks wasn’t going to cut it (at least with my models).</p>

<p>For example, the car would successfully drive through some parts of track 1, but fail horribly at other parts. At other times, the car would simply drive off the track and into the ocean, or take the dirt path after the textured bridge, which is usually unsuccessful. At other times, my model would be able to fully drive laps around track 1, and fail horribly at track 2.</p>

<p>Through my experiments, I found these obsticles to be the toughest to tackle:</p>

<div class="fig figcenter fighighlight">
<img src="/assets/behavior_cloning/fig8_hardest_turn.png" /> <div class="figcaption">
Figure 1: This is the turn where my car would drive off the track, and into the ocean most frequently.
</div>
</div>

<div class="fig figcenter fighighlight">
<img src="/assets/behavior_cloning/fig7_challenge_turn.png" /> 
<div class="figcaption">
Figure 2: This is where my car would drive into the sand part, and fail afterwards. I could have introduced "sand-driving" recordings to combat this edge case, but my current model is not supposed to drive on dirt.
</div>
</div>

<h2 id="avoiding-overfitting">Avoiding overfitting</h2>

<p>I took some measures to avoid points of overfitting as much as possible.</p>

<p><strong>Theory:</strong> I think that when using the full (160,320,3) image to drive our car, it makes it easy for our car to pick up on queues to memorize. It doesn’t take many epochs to get a model that can ‘autonomously’ drive around a single track (Trained and Evaluated on a single track). And it is still hard to quantify/interpret how much memorization was going on even when introducing heavy dropout to reduce the memorizing/overfitting.</p>

<p><strong>Proof:</strong> (At least for my model) I recorded a seperate dataset that drove through the track in the <strong><em>backwards</em></strong> direction (Inspired by Mario-Kart), and tried to see how well my model would do driving forward (regularly). My model was able to make some simple turns, but would fail very quickly driving off track. This shows us that for my particular model, it was indeed memorizing some artifacts in the environment to help it drive successfully in the previous experiment (trained and evaluated with forward driving).</p>

<p>To combat this proposed problem, I cropped off the top half of all images, leaving me with images that dont take into account environmental queues to memorize (See figure 3).</p>

<div class="fig figcenter fighighlight">
<img src="/assets/behavior_cloning/fig3_full.png" /> 
<div class="figcaption">
Figure 3: Before Crop [Shape: 160,320,3]. We have access to all environmental queues, which I argue are easily memorized 
</div>
</div>

<div class="fig figcenter fighighlight">
<img src="/assets/behavior_cloning/fig4_cropped.png" /> 
<div class="figcaption">
Figure 4: After Crop [Shape: 80,320,3]. We can see the model gets enough information of the road to make a steering decision, without having access to environmental memorization queues
</div>
</div>

<p><strong>Problems:</strong> We could argue that cropping the image is not realistic, in the sense that in the real world, we will want our models to be able to use environmental queues to make decisions. In this simulation, I make the assumption that the objective is just to safely drive around the track. And to get an honest exerpience of autonomously driving (as least memorization as possible), I decided to crop the top half of the input camera image in order to prevent memorization of environmental queues around the track.</p>

<p>Using the data from the recording session of driving through the track <strong><em>backwards</em></strong>, I trained a model with the cropped version of the images, and evaluated the model on driving forward through the track. My model was able to drive laps autonomously through the forward path. I argue that very little memorization is leveraged to successfully autonomously drive.</p>

<p>I make an (unproven) assumption that it is now “safe” to use data from driving forward, as long as its cropped.</p>

<p>To further avoid overfitting, I utilize dropout after the large fully connected layer.</p>

<h2 id="recording-session-and-hyper-parameter-details">Recording Session and hyper-parameter details</h2>

<p>Using data from 5 different recording sessions (Detailed below), I trained my model with the following parameters:</p>
<ul>
  <li>Adam Optimizer: lr=0.0001 -&gt; (15 epochs) -&gt; lr=0.00001 -&gt; (5 epochs)
    <ul>
      <li>I drop the learning rate to 0.00001 because in experiments, 15 epochs was enough to converge. And the extra epochs from the dropped learning rate were successful in dropping validaion cost.</li>
    </ul>
  </li>
</ul>

<p>5 Recording Session Details:</p>
<ul>
  <li>Recording Session 1:
    <ul>
      <li>Driving through track #1 4 times, straight driving, slalom driving, hugged left line, hugged right line.</li>
    </ul>
  </li>
  <li>Recording Session 2:
    <ul>
      <li>Driving through track #1 2 times in the backwards direction; straight and slalom driving.</li>
    </ul>
  </li>
  <li>Recording Session 3:
    <ul>
      <li>Driving down track #2 to the end, center driving.</li>
    </ul>
  </li>
  <li>Recording Session 4:
    <ul>
      <li>Driving down track #2 backwards, from the end to the start.</li>
    </ul>
  </li>
  <li>Recording Session 5:
    <ul>
      <li>Driving down track #2 forwards, hugging outter lane during sharp turns (to avoid hitting/turning into walls during sharp turns)</li>
    </ul>
  </li>
</ul>

<h1 id="training-process">Training process</h1>

<p>I leverage a small validation set (less than 5% of the data), just to monitor any possible overfitting of the dataset. I kept an eye out for a stable validation loss, when the validation loss would spike, I would assume overfitting is occuring. I trained using shuffled batches of 128. That is, we are assuming our model is making frame by frame predictions, since we train with random batches of 128 images. I started off with 0.0001 Learning rate for 15 epochs, which was enough for the cost to converge, and followed that with 5 extra epochs of a dropped 0.00001 learning rate. This further dropped the cost, as well the validation cost. After this, we exported the keras architecture and model weights and dove into the simulator.</p>

<h1 id="final-notes">Final notes</h1>
<p>My model is able to successfully drive through both tracks.</p>

<p>(NOTE: The 2nd track starts off weird, where the car struggles to move up the hill. If you wait a couple of seconds, the car will have enough force to make it up the hill and [hopefully] autonomously drive).</p>

  </article>

</div>
      </div>
    </div>
    
    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>